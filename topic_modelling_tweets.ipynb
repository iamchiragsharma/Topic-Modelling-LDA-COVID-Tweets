{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chirag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from operator import itemgetter\n",
    "nltk.download('stopwords')\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "import unicodedata\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.RESERVED, p.OPT.SMILEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STATIC VARIABLES\n",
    "\n",
    "DATA_DIR = \"COVID-19-Twitter-Indian-Data\"\n",
    "MALLET_PATH = 'mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_hourly = os.listdir(DATA_DIR)\n",
    "file_names_hourly.remove(\"metadata.csv\")\n",
    "#Mapping Files From Hourly to Daily Basis\n",
    "file_names_daily = [file_name[:-7] for file_name in file_names_hourly]\n",
    "file_names_df = pd.DataFrame({'Hourly' : file_names_hourly, 'Daily': file_names_daily})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hourly</th>\n",
       "      <th>Daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coronavirus-tweet-id-2020-05-14-17.csv</td>\n",
       "      <td>coronavirus-tweet-id-2020-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coronavirus-tweet-id-2020-04-22-17.csv</td>\n",
       "      <td>coronavirus-tweet-id-2020-04-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus-tweet-id-2020-03-29-20.csv</td>\n",
       "      <td>coronavirus-tweet-id-2020-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coronavirus-tweet-id-2020-05-05-13.csv</td>\n",
       "      <td>coronavirus-tweet-id-2020-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coronavirus-tweet-id-2020-03-22-17.csv</td>\n",
       "      <td>coronavirus-tweet-id-2020-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Hourly                            Daily\n",
       "0  coronavirus-tweet-id-2020-05-14-17.csv  coronavirus-tweet-id-2020-05-14\n",
       "1  coronavirus-tweet-id-2020-04-22-17.csv  coronavirus-tweet-id-2020-04-22\n",
       "2  coronavirus-tweet-id-2020-03-29-20.csv  coronavirus-tweet-id-2020-03-29\n",
       "3  coronavirus-tweet-id-2020-05-05-13.csv  coronavirus-tweet-id-2020-05-05\n",
       "4  coronavirus-tweet-id-2020-03-22-17.csv  coronavirus-tweet-id-2020-03-22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hourly</th>\n",
       "      <th>Daily</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corrupt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>3017</td>\n",
       "      <td>3017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Hourly  Daily\n",
       "Corrupt               \n",
       "False      3017   3017\n",
       "True         85     85"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corrupt_or_not(file_name):\n",
    "    \"\"\"Some csv files are corrupt this is a program to spot them in DATA_DIR,\n",
    "    return : True if opens False for corrupt(not open)\"\"\"\n",
    "    try:\n",
    "        pd.read_csv(os.path.join(*[DATA_DIR,file_name]))\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "file_names_df['Corrupt'] = file_names_df['Hourly'].apply(corrupt_or_not)\n",
    "file_names_df.groupby('Corrupt').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Corrupt Files and \n",
    "#Converting the Groupby object to dict such that key is the day and values are the hourly file names\n",
    "file_names_df = file_names_df[file_names_df['Corrupt'] == False]\n",
    "file_daily_hourly_map = file_names_df.groupby('Daily')['Hourly'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_full_tweets = {}\n",
    "\n",
    "# for key,files in tqdm(file_daily_hourly_map.items()):\n",
    "#     hourly_df = [pd.read_csv(os.path.join(*[DATA_DIR,file_name])) for file_name in files]\n",
    "#     daily_df = pd.concat(hourly_df)\n",
    "#     daily_df = daily_df[(daily_df['full_text'] != 'No Value Mentioned') | (daily_df['full_retweet_text'] != 'No Value Mentioned')]\n",
    "#     daily_df.loc[daily_df['full_text'] == 'No Value Mentioned','full_text'] =  daily_df.loc[daily_df['full_text'] == 'No Value Mentioned','full_retweet_text']\n",
    "#     #Forcefully type casting to str because some values were just float\n",
    "#     daily_df['full_text'] = daily_df['full_text'].astype(str).apply(p.clean)\n",
    "#     daily_full_tweets[key] = \" \".join(daily_df['full_text'].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accent_chars(text):\n",
    "    text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    \"\"\"This takes text as input and then finds whether each character is not a-z A-Z 0-9 and replaces them with nothing \"\"\"\n",
    "    pattern = r'[^a-zA-z\\s]' if not remove_digits else r'[^0-9a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def cleaner(doc):\n",
    "    return \" \".join(map(str.lower,(map(str,([token.lemma_ for token in doc if not token.is_stop | token.is_space | token.is_punct | token.like_url])))))\n",
    "\n",
    "\n",
    "def pipeline_2_tokenizer(daily_df):\n",
    "    text_data_cleaned = list(nlp.pipe(daily_df.full_text.values.tolist(),disable=[\"tagger\", \"parser\",\"ner\"]))\n",
    "    text_data_cleaned = [t for t in text_data_cleaned if t]\n",
    "    text_tokens = []\n",
    "    for doc in text_data_cleaned:\n",
    "        tokens = []\n",
    "        for t in tokenizer(doc):\n",
    "            if len(t.text) == 1 or len(list(set(t.text))) == 1:\n",
    "                pass\n",
    "            else:\n",
    "                tokens.append(t.text)\n",
    "        text_tokens.append(tokens)\n",
    "    return text_tokens\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\",max_length = 2000000)\n",
    "nlp.add_pipe(cleaner,name=\"cleaner\",first=True)\n",
    "nlp.add_pipe(remove_accent_chars,name='accent_char_removal',after='cleaner')\n",
    "nlp.add_pipe(remove_special_characters,name='remove_special_char',after='accent_char_removal')\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "    \n",
    "def single_frame(file_names):\n",
    "    \"Concatenates all dataframe from a day and returns dataframe after fixing the full_text column\"\n",
    "    hourly_df = [pd.read_csv(os.path.join(*[DATA_DIR,file_name])) for file_name in file_names]\n",
    "    daily_df = pd.concat(hourly_df)\n",
    "    daily_df = daily_df[(daily_df['full_text'] != 'No Value Mentioned') | (daily_df['full_retweet_text'] != 'No Value Mentioned')]\n",
    "    daily_df.loc[daily_df['full_text'] == 'No Value Mentioned','full_text'] =  daily_df.loc[daily_df['full_text'] == 'No Value Mentioned','full_retweet_text']\n",
    "    daily_df['full_text'] = daily_df['full_text'].astype(str).apply(p.clean)\n",
    "    return daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow_corpus(text_tokens):\n",
    "    bigram = gensim.models.Phrases(text_tokens, min_count=5, threshold=50,delimiter=b'_') # higher threshold fewer phrases.\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    norm_corpus_bigrams = [bigram_model[doc] for doc in text_tokens]\n",
    "    dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "    \n",
    "    return bow_corpus,dictionary\n",
    "\n",
    "def topic_generator(bow_corpus,dictionary,num_topics=2,chunksize=1740):\n",
    "    lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary,chunksize=1740, alpha='auto',eta='auto', random_state=42,\n",
    "                                       iterations=500, num_topics=2,passes=20, eval_every=None)\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_extraction(num_of_topics = 2, num_words_per_topic = 5):\n",
    "    daily_topic_topics = []\n",
    "    keys_without_topics = []\n",
    "    for key,file_names in tqdm(file_daily_hourly_map.items()):\n",
    "        per_day = {}\n",
    "        per_day['Key'] = key\n",
    "        daily_df = single_frame(file_names)\n",
    "        text_tokens = pipeline_2_tokenizer(daily_df)\n",
    "        bow_corpus,dictionary = generate_bow_corpus(text_tokens)\n",
    "        try:\n",
    "            lda_model = topic_generator(bow_corpus, dictionary)\n",
    "            #Adjust number of topics and words in the topics here.\n",
    "            for topic_id, topics in lda_model.print_topics(num_topics=num_of_topics, num_words=num_words_per_topic):\n",
    "                keywords = list(map(lambda x : x[1:-1], re.findall('\"\\w+\"',topics)))\n",
    "                per_day[f\"Topic {topic_id}\"] = \" \".join(keywords)\n",
    "            daily_topic_topics.append(per_day)\n",
    "        except:\n",
    "            print(key)\n",
    "    return daily_topic_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronavirus-tweet-id-2020-01-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 6/131 [01:31<38:16, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronavirus-tweet-id-2020-01-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 8/131 [03:03<1:08:21, 33.35s/it]"
     ]
    }
   ],
   "source": [
    "daily_topic_topics = topic_extraction(num_of_topics = 2, num_words_per_topic = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
